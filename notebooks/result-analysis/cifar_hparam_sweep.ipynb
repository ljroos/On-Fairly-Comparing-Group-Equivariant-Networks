{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeway:\n",
    "\n",
    "standard Adam most likely works best on CIFAR10 with Cohen architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic commands, make python reimport modules when code is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".utilities/\")\n",
    "\n",
    "from utilities.download import download_sweep\n",
    "\n",
    "# set pandas dataframe display options\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_str = \"ljroos-msc/knot-solver/odiafcc3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# make figure folder\n",
    "if not os.path.exists(\"figures\"):\n",
    "    os.makedirs(\"figures\")\n",
    "\n",
    "SAVE_FOLDER = \"csv-files\"\n",
    "\n",
    "SWEEPS = {\"cifar10_hparams_tune\": \"nsn5yc39\"}\n",
    "\n",
    "full_dfs = {}\n",
    "for dataset, sweep in SWEEPS.items():\n",
    "    print(f\"Downloading {dataset}\")\n",
    "    sweep_id = f\"ljroos-msc/knot-solver/{sweep}\"\n",
    "    save_loc = os.path.join(SAVE_FOLDER, f\"{dataset}_{sweep}.csv\")\n",
    "    _ = download_sweep(sweep_id, save_loc, override_existing=False)\n",
    "\n",
    "    full_dfs[dataset] = pd.read_csv(save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded data from https://wandb.ai/ljroos-msc/mosaic/sweeps/w705aehx/table?workspace=user-luro\n",
    "# not sure if link will work for others.\n",
    "# The API is very slow to download.\n",
    "\n",
    "# code to link different notebooks\n",
    "df = full_dfs[\"cifar10_hparams_tune\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameter df\n",
    "hparams = [\"beta1\", \"beta2\", \"learning_rate\", \"weight_decay\"]\n",
    "hparam_df = df[hparams + [\"val_loss\", \"val_accuracy\", \"test_accuracy\"]].dropna()\n",
    "hparam_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def decision_tree(cols, target, df=hparam_df):\n",
    "    X = df[cols]\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = DecisionTreeRegressor(min_samples_leaf=10)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Decision Tree Mean Squared Error: {mse}\")\n",
    "\n",
    "    # predict baseline error as mean of y_train\n",
    "    baseline = np.mean(y_train)\n",
    "    baseline_mse = mean_squared_error(y_test, np.full_like(y_test, baseline))\n",
    "    print(f\"Baseline Mean Squared Error: {baseline_mse}\")\n",
    "\n",
    "    # report mean percentage errors\n",
    "    # decision tree\n",
    "    mpe = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    print(f\"Mean Percentage Error: {mpe}\")\n",
    "\n",
    "    # baseline\n",
    "    mpe_baseline = np.mean(np.abs((y_test - baseline) / y_test)) * 100\n",
    "    print(f\"Baseline Mean Percentage Error: {mpe_baseline}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tree_model = decision_tree(hparams, \"val_loss\", hparam_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print nodes in tree_model\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "r = export_text(tree_model, feature_names=hparams)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through nodes in tree_model, and print the mean squared error for each node\n",
    "# this is a measure of the variance in the data at each node\n",
    "\n",
    "# find leaf index with lowest prediction value\n",
    "preds = tree_model.predict(hparam_df[hparams])\n",
    "appls = tree_model.apply(hparam_df[hparams])\n",
    "\n",
    "min_pred_obs = np.argmin(tree_model.predict(hparam_df[hparams]))\n",
    "print(f\"min pred value: {preds[min_pred_obs]}\")\n",
    "min_apply_obs = appls[min_pred_obs]\n",
    "\n",
    "print(min_pred_obs)\n",
    "print(f\"min node number: {min_apply_obs}\")\n",
    "\n",
    "# find indices with apply value equal to min_apply_obs\n",
    "min_indices = np.where(appls == min_apply_obs)\n",
    "\n",
    "# number of indices in leaf\n",
    "print(f\"num indices in min leaf: {len(min_indices[0])}\")\n",
    "\n",
    "# take average values of df at these indices\n",
    "min_df = hparam_df.iloc[min_indices]\n",
    "\n",
    "# print mean values of df at these indices\n",
    "print(min_df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10:\n",
    "\n",
    "# min pred value: 0.6864680647850037\n",
    "# 41\n",
    "# min node number: 35\n",
    "# num indices in min leaf: 15\n",
    "# beta1                0.804736\n",
    "# beta2                0.993898\n",
    "# log_learning_rate   -5.205313\n",
    "# log_weight_decay    -4.909954\n",
    "# log_eps             -7.989710\n",
    "# val_loss             0.690507\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for a, hparam in zip(ax.flatten(), hparam_df.columns):\n",
    "    if hparam in [\"val_loss\", \"val_accuracy\", \"test_accuracy\"]:\n",
    "        continue\n",
    "    for metric in [\"val_loss\", \"val_accuracy\", \"test_accuracy\"]:\n",
    "        a.scatter(\n",
    "            x=hparam_df[hparam], y=hparam_df[metric], label=metric, s=15, alpha=0.5\n",
    "        )\n",
    "        a.set_title(label=hparam)\n",
    "    a.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(\n",
    "    x=hparam_df[\"learning_rate\"],\n",
    "    y=hparam_df[\"val_accuracy\"],\n",
    "    label=\"train\",\n",
    "    s=15,\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.scatter(\n",
    "    x=hparam_df[\"learning_rate\"],\n",
    "    y=hparam_df[\"test_accuracy\"],\n",
    "    label=\"test\",\n",
    "    s=15,\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(\"train / val accuracies\")\n",
    "ax.set_xlabel(\"learning rate\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define predictor columns and target column\n",
    "predictor_columns = [\"beta1\", \"beta2\", \"learning_rate\", \"weight_decay\"]\n",
    "target_column = \"val_loss\"\n",
    "\n",
    "# Extract the predictors and target\n",
    "X = hparam_df[predictor_columns]\n",
    "y = hparam_df[target_column]\n",
    "\n",
    "# Reset index to ensure alignment\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Get feature names for the polynomial features\n",
    "feature_names = poly.get_feature_names_out(predictor_columns)\n",
    "\n",
    "# Create a DataFrame for the polynomial features\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=feature_names)\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_poly_df = sm.add_constant(X_poly_df)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X_poly_df).fit()\n",
    "\n",
    "# Output the regression summary\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
